{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA-DebIb_iQq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset,SequentialSampler,RandomSampler,DataLoader\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyGyr_4V_iQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7e7877-c754-4653-efe6-52037d9a0332"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BoUkNozzyo3"
      },
      "outputs": [],
      "source": [
        "path=\" \"\n",
        "data=pd.read_excel(path)\n",
        "data['label']=data['label']\n",
        "data['text']=data['rad']\n",
        "train,test=train_test_split(data,test_size=0.2,stratify=data['label'],random_state=RANDOM_SEED)\n",
        "val,test=train_test_split(test,test_size=0.3,stratify=data['label'],random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8EFPqBW_iQ-"
      },
      "outputs": [],
      "source": [
        "encoding=tokenizer.encode_plus(\n",
        "    sample_txt,\n",
        "    max_length=32,\n",
        "    add_special_tokens=True,# [CLS]和[SEP]\n",
        "    return_token_type_ids=True,\n",
        "    pad_to_max_length=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',# Pytorch tensor张量\n",
        "\n",
        ")\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNur91G3_iRC"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in tqdm(train['text']):\n",
        "    tokens = tokenizer.encode(txt, max_length=512)\n",
        "    token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK90Bpin_iRC"
      },
      "outputs": [],
      "source": [
        "sns.set()\n",
        "plt.figure(dpi=300)\n",
        "sns.distplot(token_lens,color='b')\n",
        "plt.xlim([0, 1000]);\n",
        "plt.xlabel('Token length');\n",
        "plt.ylabel('Kernal Density Estimation');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhgd5gtP_iRE"
      },
      "outputs": [],
      "source": [
        "class ReportDataset(Dataset):\n",
        "    def __init__(self,texts,labels,tokenizer,max_len):\n",
        "        self.texts=texts\n",
        "        self.labels=labels\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_len=max_len\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "\n",
        "        text=str(self.texts[item])\n",
        "        label=self.labels[item]\n",
        "        \n",
        "        encoding=self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        \n",
        "#         print(encoding['input_ids'])\n",
        "        return {\n",
        "            'texts':text,\n",
        "            'input_ids':encoding['input_ids'].flatten(),\n",
        "            'attention_mask':encoding['attention_mask'].flatten(),\n",
        "            'labels':torch.tensor(label,dtype=torch.long)\n",
        "        }\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OqmQfGh_iRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a02dab5-eec7-4dd9-d442-a820b1d5327c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((982, 7), (123, 7), (123, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "df_train, df_test = train_test_split(train, test_size=0.2, stratify=train['label'],random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, stratify=df_test['label'],random_state=RANDOM_SEED)\n",
        "df_train.shape, df_val.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OqrM7Qb_iRF"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df,tokenizer,max_len,batch_size):\n",
        "    ds=ReportDataset(\n",
        "        texts=df['text'].values,\n",
        "        labels=df['label'].values,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    \n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "#         num_workers=4 # windows多线程\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2nYIrPN_iRG"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_data_loader = create_data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHj02fJP_iRM"
      },
      "outputs": [],
      "source": [
        "class ReportClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(ReportClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict = False\n",
        "        )\n",
        "        output = self.drop(pooled_output) # dropout\n",
        "        return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nr_ag4k_iRN"
      },
      "outputs": [],
      "source": [
        "model = PaperClassifier(len(class_names))\n",
        "model = model.to(device)\n",
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9xlaznb_iRO"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cax4W_ls_iRP"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for d in tqdm(data_loader):\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"labels\"].to(device)\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngt1Kzcc_iRP"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval() # 验证预测模式\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGLpDSH8_iRP"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = defaultdict(list) # 记录10轮loss和acc\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        len(df_train)\n",
        "    )\n",
        "\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model,\n",
        "        val_data_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(df_val)\n",
        "    )\n",
        "\n",
        "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "    print()\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    if val_acc > best_accuracy:\n",
        "        torch.device('cpu')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/bert-best_model_state.bin')\n",
        "        best_accuracy = val_acc\n",
        "        torch.device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c99z2cSx_iRQ"
      },
      "source": [
        "### 8 模型评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8tbEbNH_iRR"
      },
      "outputs": [],
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device,\n",
        "  len(df_test)\n",
        ")\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM2VfhPv_iRR"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "    model = model.eval()\n",
        "\n",
        "    texts = []\n",
        "    predictions = []\n",
        "    prediction_probs = []\n",
        "    real_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            texts = d[\"texts\"]\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "            texts.extend(texts)\n",
        "            predictions.extend(preds)\n",
        "            prediction_probs.extend(probs)\n",
        "            real_values.extend(targets)\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu()\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "    real_values = torch.stack(real_values).cpu()\n",
        "    return texts, predictions, prediction_probs, real_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7thzaZxY_iRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e50e3c4-0b2d-4e39-f015-fcd326e21bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "y_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLMTwnwF_iRS"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=[str(label) for label in class_names]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFEsHSrO_iRS"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label');\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFa5h3mJ_iRV"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "from transformers import (AutoModelForMaskedLM,\n",
        "                          AutoTokenizer, LineByLineTextDataset,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          Trainer, TrainingArguments)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train_data = pd.read_csv(path)\n",
        "test_data = pd.read_csv(path)\n",
        "data = pd.concat([train_data, test_data])\n",
        "data.apply(lambda x: x.replace('\\n', ''))\n",
        "\n",
        "text = '\\n'.join(data.text.tolist())\n",
        "\n",
        "with open('text.txt', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "model_name = ' '\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.save_pretrained(model_name)\n",
        "\n",
        "train_dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\" \", \n",
        "    block_size=256)\n",
        "\n",
        "valid_dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\" \",  \n",
        "    block_size=256)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\" \",  \n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    evaluation_strategy='steps',\n",
        "    save_total_limit=2,\n",
        "    eval_steps=200,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    greater_is_better=False,\n",
        "    load_best_model_at_end=True,\n",
        "    prediction_loss_only=True,\n",
        "    report_to=\"none\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset)\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(f' ')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "“How to Fine-Tune BERT for Text Classification.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [
        "31doAPBB_iQ4",
        "7KEZ8rq-_iRE",
        "c99z2cSx_iRQ",
        "EEVBnT4F_iRT"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}